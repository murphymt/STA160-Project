{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import lxml.html as html\n",
    "#import time\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "wd0 = \"/Users/tmm/Documents/GitHub/STA160-Project/STA160-Jobs-Market-Project/Working/data_collection\"\n",
    "os.chdir(wd0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shell_crawler(search, search_terms, num_pages):\n",
    "    \"\"\"\n",
    "    (Purpose)\n",
    "    Primary crawling function that search key job terms on the cybercoders webpage and extracts\n",
    "    all of the job titles and their corresponding urls. This is done so that each of the listings\n",
    "    on each page of the search terms are known and then helper functions may be used to crawl through\n",
    "    each one and retrieve the relevant information. \n",
    "    \n",
    "    (Arguments)\n",
    "    search : list of broad data/statistics related search terms to request from the the main webpage\n",
    "    search_terms : list of the specific terms for each search\n",
    "    num_pages : list of number of pages give each search term\n",
    "    \n",
    "    (Returns)\n",
    "    job_titles : list of the job titles corresponding to each listing of the search results\n",
    "    job_links : list of the urls corresponsing to each listing of the search results\n",
    "    job_type : the \"statistics\" related search terms that were \"techinically\" inputted into the cybercoder search box\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # query each job type (data scientist, data engineer, data analyst, business intelligence, finance, product manager ), \n",
    "    # crawl through each page, extract (titles, links, search term used)\n",
    "    job_type = []\n",
    "    job_titles = []\n",
    "    job_links = []\n",
    "    total_response = []\n",
    "    flattened_set = []\n",
    "  \n",
    "    for j in range(num_pages + 1):\n",
    "        j = j + 1\n",
    "        url = search + '&page=' + str(j)\n",
    "        response = requests.get(url)\n",
    "        response_tree = html.fromstring(response.content)\n",
    "        title_elements = response_tree.xpath('//div[@class=\"job-title\"]/a')\n",
    "        total_response.append(title_elements)\n",
    "        \n",
    "    for sub_response in total_response:\n",
    "        for val in sub_response:\n",
    "            flattened_set.append(val)\n",
    "                  \n",
    "    for title_tag in flattened_set:\n",
    "        title = title_tag.text_content().strip()\n",
    "        link = title_tag.items()[0][1]\n",
    "        job_titles.append(title)\n",
    "        job_links.append('https://www.cybercoders.com' + link)\n",
    "        job_type.append(search_terms)\n",
    "        \n",
    "    return job_titles, job_links, job_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scientist_url = 'https://www.cybercoders.com/search/?searchterms=Data+Scientist&searchlocation=&newsearch=true&originalsearch=true&sorttype=relevance'\n",
    "data_engineer_url = 'https://www.cybercoders.com/search/?searchterms=Data+Engineer&searchlocation=&newsearch=true&originalsearch=true&sorttype=relevance'\n",
    "data_analyst_url = 'https://www.cybercoders.com/search/?searchterms=Data+Analyst&searchlocation=&newsearch=true&originalsearch=true&sorttype=relevance'\n",
    "business_intelligence_url = 'https://www.cybercoders.com/search/?searchterms=Business+Intelligence+&searchlocation=&newsearch=true&originalsearch=true&sorttype=relevance'\n",
    "\n",
    "search = [data_scientist_url, data_engineer_url, data_analyst_url, business_intelligence_url] # append base urls into a list\n",
    "search_terms = ['Data Scientist', 'Data Engineer', 'Data Analyst', 'Business Intelligence'] # define search terms\n",
    "\n",
    "# the number of search results divided by 20 listings per pages which is equal to the number of pages and then round up\n",
    "# adjust the numerator is needed\n",
    "num_pages = [int(math.ceil(84 / 20)), int(math.ceil(370 / 20)), int(math.ceil(52 / 20)), int(math.ceil(61 / 20))] # number of pages per search term (number of results divided by 20 listings per page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA SCIENTIST DROP DUPLICATES\n",
    "data_scientist_crawl = shell_crawler(search[0], search_terms[0], num_pages[0])\n",
    "data_scientist_titles = data_scientist_crawl[0]\n",
    "data_scientist_links = data_scientist_crawl[1]\n",
    "data_scientist_type = data_scientist_crawl[2]\n",
    "ds_check_cols = ['Search', 'Title', 'URL']\n",
    "ds_check = pd.DataFrame({'Search' : data_scientist_type,\n",
    "                         'Title' : data_scientist_titles, \n",
    "                         'URL' : data_scientist_links}, columns = ds_check_cols)\n",
    "\n",
    "ds_check = ds_check.drop_duplicates(subset = 'URL', keep = False)\n",
    "data_scientist_titles = list(ds_check['Title'])\n",
    "data_scientist_links = list(ds_check['URL'])\n",
    "data_scientist_type = list(ds_check['Search'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA ENGINEER DROP DRUPLICATES\n",
    "data_engineer_crawl = shell_crawler(search[1], search_terms[1], num_pages[1])\n",
    "data_engineer_titles = data_engineer_crawl[0]\n",
    "data_engineer_links = data_engineer_crawl[1]\n",
    "data_engineer_type = data_engineer_crawl[2]\n",
    "de_check_cols = ['Search', 'Title', 'URL']\n",
    "de_check = pd.DataFrame({'Search' : data_engineer_type,\n",
    "                         'Title' : data_engineer_titles, \n",
    "                         'URL' : data_engineer_links}, columns = de_check_cols)\n",
    "\n",
    "de_check = de_check.drop_duplicates(subset = 'URL', keep = False)\n",
    "data_engineer_titles = list(de_check['Title'])\n",
    "data_engineer_links = list(de_check['URL'])\n",
    "data_engineer_type = list(de_check['Search'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA ANALYST DROP DUPLICATES\n",
    "data_analyst_crawl = shell_crawler(search[2], search_terms[2], num_pages[2])\n",
    "data_analyst_titles = data_analyst_crawl[0]\n",
    "data_analyst_links = data_analyst_crawl[1]\n",
    "data_analyst_type = data_analyst_crawl[2]\n",
    "da_check_cols = ['Search', 'Title', 'URL']\n",
    "da_check = pd.DataFrame({'Search' : data_analyst_type,\n",
    "                         'Title' : data_analyst_titles, \n",
    "                         'URL' : data_analyst_links}, columns = da_check_cols)\n",
    "\n",
    "da_check = da_check.drop_duplicates(subset = 'URL', keep = False)\n",
    "data_analyst_titles = list(da_check['Title'])\n",
    "data_analyst_links = list(da_check['URL'])\n",
    "data_analyst_type = list(da_check['Search'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUSINESS INTELLIGENCE DROP DUPLICATES\n",
    "business_intelligence_crawl = shell_crawler(search[3], search_terms[3], num_pages[3])\n",
    "business_intelligence_titles = business_intelligence_crawl[0]\n",
    "business_intelligence_links = business_intelligence_crawl[1]\n",
    "business_intelligence_type = business_intelligence_crawl[2]\n",
    "bi_check_cols = ['Search', 'Title', 'URL']\n",
    "bi_check = pd.DataFrame({'Search' : business_intelligence_type,\n",
    "                         'Title' : business_intelligence_titles, \n",
    "                         'URL' : business_intelligence_links}, columns = bi_check_cols)\n",
    "\n",
    "bi_check = bi_check.drop_duplicates(subset = 'URL', keep = False)\n",
    "business_intelligence_titles = list(bi_check['Title'])\n",
    "business_intelligence_links = list(bi_check['URL'])\n",
    "business_intelligence_type = list(bi_check['Search'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles_raw = data_scientist_titles + data_engineer_titles + data_analyst_titles + business_intelligence_titles \n",
    "job_links_raw = data_scientist_links + data_engineer_links + data_analyst_links + business_intelligence_links \n",
    "job_type_raw = data_scientist_type + data_engineer_type + data_analyst_type + business_intelligence_type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_links_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_type_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "dcheck_cols = ['Search', 'Title', 'URL']\n",
    "dcheck_df = pd.DataFrame({'Search' : job_type_raw,\n",
    "                          'Title': job_titles_raw,\n",
    "                          'URL' : job_links_raw}, columns = dcheck_cols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates conditioned on the URLS\n",
    "dcheck_df = dcheck_df.drop_duplicates(subset = 'URL', keep = False)\n",
    "job_titles = list(dcheck_df['Title'])\n",
    "job_links = list(dcheck_df['URL'])\n",
    "job_type = list(dcheck_df['Search'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location(newtree, location_list):\n",
    "    \"\"\"\n",
    "    (Purpose)\n",
    "    Helper function for extracting the location corresponding to each job listing.\n",
    "    This is called from the \"job_crawler\" function. \n",
    "    \n",
    "    (Arguments)\n",
    "    newtree : html tree for each job listing\n",
    "    \n",
    "    (Returns)\n",
    "    location_list : list of locations for each of the listings\n",
    "    \"\"\"\n",
    "    loelements = newtree.xpath('//div[@class=\"location\"]')[0]\n",
    "    locationcontent = loelements.text_content().strip()\n",
    "    location_list.append(locationcontent)\n",
    "    location_list =  ''.join(str(i) for i in location_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wage(newtree, wage_list):\n",
    "    \"\"\"\n",
    "   (Purpose)\n",
    "    Helper function for extracting the wage corresponding to each job listing.\n",
    "    This is called from the \"job_crawler\" function. \n",
    "    \n",
    "    (Arguments)\n",
    "    newtree : html tree for each job listing\n",
    "    \n",
    "    (Returns)\n",
    "    wage_list : list of wages for each of the listings\n",
    "    \"\"\"\n",
    "    waelements = newtree.xpath('//div[@class=\"wage\"]')[0]\n",
    "    wagecontent = waelements.text_content().strip()\n",
    "    wagecontent = re.findall('\\d+', wagecontent )\n",
    "    wage_list.append(wagecontent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description(newtree, description_list):\n",
    "    \"\"\"\n",
    "    (Purpose)\n",
    "    Helper function for extracting the description corresponding to each job listing.\n",
    "    This is called from the \"job_crawler\" function. \n",
    "    \n",
    "    (Arguments)\n",
    "    newtree : html tree for each job listing\n",
    "    \n",
    "    (Returns)\n",
    "    description_list : list of descriptions for each of the listings\n",
    "    \"\"\"\n",
    "    jdelements = newtree.xpath('//div[@class=\"job-details\"]')[0]\n",
    "    detailcontent = jdelements.text_content().strip()\n",
    "    description_list.append(detailcontent)\n",
    "    description_list = ''.join(str(i) for i in description_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skills(newtree, skills_list):\n",
    "    \"\"\"\n",
    "    (Purpose)\n",
    "    Helper function for extracting the skills corresponding to each job listing.\n",
    "    This is called from the \"job_crawler\" function. \n",
    "    \n",
    "    (Arguments)\n",
    "    newtree : html tree for each job listing\n",
    "    \n",
    "    (Returns)\n",
    "    skills_list : list of skills required for each of the listings\n",
    "    \"\"\"\n",
    "    preferelements = newtree.xpath('//div[@class=\"skills-section\"]')[0]\n",
    "    prefercontent = preferelements.text_content().strip(' \\t\\n\\r')\n",
    "    skills_list.append(prefercontent)\n",
    "    skills_list = ''.join(str(i) for i in skills_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now have job titles on each page and their links\n",
    "# Iterate through each page, crawl through each listing, and extract (location, wages, descriptions, skills)\n",
    "\n",
    "def job_crawler(job_links):\n",
    "    \"\"\"\n",
    "    (Purpose)\n",
    "    Using the links returned from the \"shell_crawler\" function, this function extracts the \n",
    "    details relating to each job listing. \n",
    "    \n",
    "    (Arguments)\n",
    "    job_links : list of the urls corresponsing to each listing of the search results\n",
    "    \n",
    "    (Returns)\n",
    "    location_list : list of locations for each of the listings\n",
    "    wage_list : list of wages for each of the listings\n",
    "    description_list : list of descriptions for each of the listings\n",
    "    skills_list : list of skills required for each of the listings\n",
    "    \"\"\"\n",
    "    \n",
    "    location_list = []\n",
    "    wage_list = []\n",
    "    description_list = []\n",
    "    skills_list = []\n",
    "    \n",
    "    for url in job_links:\n",
    "        response = requests.get(url)\n",
    "        response_tree = html.fromstring(response.content)\n",
    "        \n",
    "        location(response_tree, location_list) # call location helper function\n",
    "        wage(response_tree, wage_list) # call wage helper function\n",
    "        description(response_tree, description_list) # call description helper function\n",
    "        skills(response_tree, skills_list) # call skills helper funciton\n",
    "        #time.sleep(0.15)\n",
    "    \n",
    "    return location_list, wage_list, description_list, skills_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_cords(jobs_df, field):\n",
    "    \n",
    "    latitude = []\n",
    "    longitude = []\n",
    "    \n",
    "    nrows, ncols = jobs_df.shape\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        address = jobs_df[field][i]\n",
    "        api_key = \"AIzaSyA_bzUIipMaoeuqNQkm_62YaVK-YZgOKFc\"\n",
    "        api_response = requests.get('https://maps.googleapis.com/maps/api/geocode/json?address={0}&key={1}'.format(address, api_key))\n",
    "        api_response_dict = api_response.json()\n",
    "        i +=1\n",
    "        if api_response_dict['status'] == 'OK':\n",
    "            lat = api_response_dict['results'][0]['geometry']['location']['lat']\n",
    "            lon = api_response_dict['results'][0]['geometry']['location']['lng']\n",
    "            \n",
    "            latitude.append(lat)\n",
    "            longitude.append(lon)\n",
    "    \n",
    "    return latitude, longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the title and links crawl through the listings and extract the key information\n",
    "listing_crawl = job_crawler(job_links)\n",
    "location_list = listing_crawl[0]\n",
    "wage_list = listing_crawl[1]\n",
    "description_list = listing_crawl[2]\n",
    "skills_list = listing_crawl[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final dataframe\n",
    "df_columns = ['Search', 'Title','Location','Salary','Description','Skills','URL']\n",
    "raw_jobs_data_df = pd.DataFrame({'Search' : job_type,\n",
    "                                 'Title': job_titles,\n",
    "                                 'Location' : location_list, \n",
    "                                 'Salary' : wage_list, \n",
    "                                 'Description' : description_list, \n",
    "                                 'Skills' : skills_list, \n",
    "                                 'URL' : job_links}, columns = df_columns)\n",
    "\n",
    "geo_list = geo_cords(raw_jobs_data_df, 'Location')\n",
    "raw_jobs_data_df['Latitude'] = geo_list[0]\n",
    "raw_jobs_data_df['Longitude'] = geo_list[1]\n",
    "raw_jobs_data_df = raw_jobs_data_df[['Search','Title','Description', 'Skills', 'Location', 'Latitude', 'Longitude', 'Salary', 'URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = '/Users/tmm/Documents/GitHub/STA160-Project/STA160-Jobs-Market-Project/Working/data'\n",
    "os.chdir(w1)\n",
    "raw_jobs_data_df.to_csv('raw_jobs_data.csv', index = False)\n",
    "raw_jobs_out = open('raw_jobs_data.pickle', 'wb')\n",
    "pickle.dump(raw_jobs_data_df, raw_jobs_out)\n",
    "raw_jobs_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
