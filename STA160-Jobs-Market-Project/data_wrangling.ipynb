{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "wd2 = \"/Users/tmm/Documents/GitHub/STA160-Project/Working/cyber_coders/Data\"\n",
    "os.chdir(wd2)\n",
    "\n",
    "jobs_scrub_in = open('raw_jobs_data.pickle', 'rb')\n",
    "jobs_scrub = pickle.load(jobs_scrub_in)\n",
    "\n",
    "\n",
    "jobs_scrub.columns = ['Search','Title','Description', 'Skills', 'Location', 'Latitude', 'Longitude', 'Salary', 'URL']\n",
    "\n",
    "wd3 = \"/Users/tmm/Documents/GitHub/STA160-Project/Working/cyber_coders/2data_wrangling\"\n",
    "os.chdir(wd3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def description_scrub(jobs_scrub, field):\n",
    "    \"\"\"\n",
    "    (Purpose)\n",
    "    Cleans up the description for each listing. The raw data has '*', '\\r', \\n', and \n",
    "    'Applicants must be authorized to work in the U.S.' that need to be removed in order \n",
    "    to have a cleaner format. \n",
    "    \n",
    "    (Arguments)\n",
    "    field : Description\n",
    "        \n",
    "    (Returns)\n",
    "    description : list of scrubbed descriptions\n",
    "    \n",
    "    \"\"\"\n",
    "    nrows, ncols = jobs_scrub.shape\n",
    "    \n",
    "    removals = ['*','\\r','\\n','Applicants must be authorized to work in the U.S.']\n",
    "    for i in range(nrows):\n",
    "        for j in removals:\n",
    "            holder = jobs_scrub[field][i].replace(j,'')\n",
    "            jobs_scrub.loc[i,field] = holder\n",
    "    return jobs_scrub[field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skills_clean(jobs_scrub, field):\n",
    "    \"\"\"\n",
    "    (Purpose)\n",
    "    Cleans up the skills for each listing. The raw data has 'Preferred Skills', '\\r', and \\n'\n",
    "    that need to be removed in order to have a cleaner format. \n",
    "    \n",
    "    (Arguments)\n",
    "    field : Skills\n",
    "    \n",
    "    (Returns)\n",
    "    skills : list of scrubbed skills\n",
    "    \n",
    "    \"\"\"\n",
    "    nrows, ncols = jobs_scrub.shape\n",
    "    \n",
    "    removals1 = ['Preferred Skills', '\\r', '\\n']\n",
    "    for i in range(nrows):\n",
    "        for j in removals1:\n",
    "            holder1 = jobs_scrub[field][i].replace(j,'')\n",
    "            jobs_scrub.loc[i,field] = holder1\n",
    "        #jobs_scrub.loc[i,field] = jobs_scrub.loc[i,field].split('  ')\n",
    "    \n",
    "    skills_split = lambda jobs_scrub : jobs_scrub[field].split('  ')\n",
    "    jobs_scrub[field] = jobs_scrub.apply(skills_split, axis = 1)\n",
    "    \n",
    "    none_removal = lambda jobs_scrub : list(filter(None, jobs_scrub[field]))\n",
    "    jobs_scrub[field] = jobs_scrub.apply(none_removal, axis = 1)\n",
    "    \n",
    "    \n",
    "    return jobs_scrub[field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_clean(jobs_scrub, field):\n",
    "    \"\"\"\n",
    "    (Purpose)\n",
    "    Seperates the locations for each listing into city and states, which is a cleaner format for\n",
    "    conducting analysis later on. \n",
    "    \n",
    "    (Arguments)\n",
    "    field : Location\n",
    "    \n",
    "    (Return)\n",
    "    city : list of cities for job listings\n",
    "    state : list of state for job listings\n",
    "    \n",
    "    \"\"\"\n",
    "    nrows, ncols = jobs_scrub.shape\n",
    "    cities = []\n",
    "    states = []\n",
    "    for i in range(nrows):\n",
    "        location_split = jobs_scrub[field][i].split(', ')\n",
    "        cities.append(location_split[0])\n",
    "        states.append(location_split[1])\n",
    "    return cities, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_clean(jobs_scrub, field):\n",
    "    \"\"\"\n",
    "    (Purpose)\n",
    "    Cleans up the salary portion for each listing if there is data present. Seperates the salary\n",
    "    estimates into a minimum and maximum salary estimate. Also, the format is converted into thousands\n",
    "    and numeric. \n",
    "    \n",
    "    (Arguments)\n",
    "    field : Salary\n",
    "    \n",
    "    (Return)\n",
    "    min_salary : list of minimum salary estimates\n",
    "    max_salary : list of maximum salary estimates\n",
    "    \n",
    "    \"\"\"\n",
    "    nrows, ncols = jobs_scrub.shape\n",
    "    min_salary = []\n",
    "    max_salary = []\n",
    "    mean_salary = []\n",
    "\n",
    "        \n",
    "    for i in range(nrows):\n",
    "        for j in range(len(jobs_scrub[field][i])):\n",
    "            if len(jobs_scrub[field][i][j]) >= 2:\n",
    "                jobs_scrub[field][i][j] = jobs_scrub[field][i][j] + '000'\n",
    "                jobs_scrub[field][i][j] = int(jobs_scrub[field][i][j])\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        if len(jobs_scrub[field][i]) == 0:\n",
    "            jobs_scrub[field][i].append(0)\n",
    "            jobs_scrub[field][i].append(0)\n",
    "    \n",
    "    for i in range(nrows):\n",
    "        min_salary.append(jobs_scrub[field][i][0])\n",
    "        max_salary.append(jobs_scrub[field][i][1])\n",
    "        mean_salary.append((jobs_scrub[field][i][0] + jobs_scrub[field][i][1]) / 2)\n",
    "        \n",
    "    for i in range(nrows):\n",
    "        if min_salary[i] == 0:\n",
    "            min_salary[i] = 'Unknown'\n",
    "        if max_salary[i] == 0:\n",
    "            max_salary[i] = 'Unknown'\n",
    "        if mean_salary[i] ==0:\n",
    "            mean_salary[i] = 'Unknown'\n",
    "            \n",
    "            \n",
    "    return min_salary, max_salary, mean_salary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_list = description_scrub(jobs_scrub, 'Description')\n",
    "skills_list = skills_clean(jobs_scrub, 'Skills')\n",
    "locations_list = location_clean(jobs_scrub, 'Location')\n",
    "wage_list = salary_clean(jobs_scrub, 'Salary')\n",
    "\n",
    "jobs_scrub['Description'] = description_list\n",
    "jobs_scrub['Skills'] = skills_list\n",
    "jobs_scrub['Cities'] = locations_list[0]\n",
    "jobs_scrub['States'] = locations_list[1]\n",
    "jobs_scrub['Min_Salary'] = wage_list[0]\n",
    "jobs_scrub['Max_Salary'] = wage_list[1]\n",
    "jobs_scrub['Mean_Salary'] = wage_list[2]\n",
    "jobs_scrub = jobs_scrub[jobs_scrub['Description'] != '']\n",
    "\n",
    "jobs_scrub = jobs_scrub[['Search', \n",
    "                         'Title',\n",
    "                         'Description',\n",
    "                         'Skills',\n",
    "                         'Location',\n",
    "                         'Cities', \n",
    "                         'States', \n",
    "                         'Latitude', \n",
    "                         'Longitude', \n",
    "                         'Min_Salary', \n",
    "                         'Max_Salary', \n",
    "                         'Mean_Salary', \n",
    "                         'URL']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy cleaned up jobs_data\n",
    "master_data = jobs_scrub.copy()\n",
    "\n",
    "# dataframe for data scientist\n",
    "data_scientist_df = master_data.copy()\n",
    "data_scientist_df = data_scientist_df[data_scientist_df['Search'].str.contains('Data Scientist')]\n",
    "data_scientist_df = data_scientist_df.reset_index(drop = True)\n",
    "\n",
    "# dataframe for data engineer\n",
    "data_engineer_df = master_data.copy()\n",
    "data_engineer_df = data_engineer_df[data_engineer_df['Search'].str.contains('Data Engineer')]\n",
    "data_engineer_df = data_engineer_df.reset_index(drop = True)\n",
    "\n",
    "# dataframe for data analyst\n",
    "data_analyst_df = master_data.copy()\n",
    "data_analyst_df = data_analyst_df[data_analyst_df['Search'].str.contains('Data Analyst')]\n",
    "data_analyst_df = data_analyst_df.reset_index(drop = True)\n",
    "\n",
    "# dataframe for business intelligence\n",
    "business_intelligence_df = master_data.copy()\n",
    "business_intelligence_df = business_intelligence_df[business_intelligence_df['Search'].str.contains('Business Intelligence')]\n",
    "business_intelligence_df = business_intelligence_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_data.to_csv('jobs_data.csv',index = False)\n",
    "data_scientist_df.to_csv('data_scientist_data.csv', index = False)\n",
    "data_engineer_df.to_csv('data_engineer_data.csv', index = False)\n",
    "data_analyst_df.to_csv('data_analyst_data.csv', index = False)\n",
    "business_intelligence_df.to_csv('BI_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w4 = '/Users/tmm/Documents/GitHub/STA160-Project/Working/cyber_coders/Data'\n",
    "os.chdir(w4)\n",
    "\n",
    "master_data_out = open('jobs_data.pickle', 'wb')\n",
    "pickle.dump(master_data, master_data_out)\n",
    "master_data_out.close()\n",
    "\n",
    "data_scientist_out = open('data_scientist_data.pickle', 'wb')\n",
    "pickle.dump(data_scientist_df, data_scientist_out)\n",
    "data_scientist_out.close()\n",
    "\n",
    "data_engineer_out = open('data_engineer_data.pickle', 'wb')\n",
    "pickle.dump(data_engineer_df, data_engineer_out)\n",
    "data_engineer_out.close()\n",
    "\n",
    "data_analyst_out = open('data_analyst_data.pickle', 'wb')\n",
    "pickle.dump(data_analyst_df, data_analyst_out)\n",
    "data_analyst_out.close()\n",
    "\n",
    "BI_out = open('BI_data.pickle', 'wb')\n",
    "pickle.dump(business_intelligence_df, BI_out)\n",
    "BI_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
